{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinpella/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os, string, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import snowballstemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input \n",
    "from keras.layers import GlobalMaxPooling1D, concatenate, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}/data/Airline-Sentiment-2-w-AA.csv', usecols=['text', 'airline_sentiment'], encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical label class into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc = TextCleaner()\n",
    "df['clean_text'] = tc.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized'] = df['clean_text'].apply(lambda row: tokenizer(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>(what, said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>(plus, youve, added, commercials, to, the, experience, tacky)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>(i, didnt, today, must, mean, i, need, to, take, another, trip)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>(its, really, aggressive, to, blast, obnoxious, entertainment, in, your, guests, faces, amp, they, have, little, recourse)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>(and, its, a, really, big, bad, thing, about, it)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "                                                                                                                    tokenized  \n",
       "0  (what, said)                                                                                                                \n",
       "1  (plus, youve, added, commercials, to, the, experience, tacky)                                                               \n",
       "2  (i, didnt, today, must, mean, i, need, to, take, another, trip)                                                             \n",
       "3  (its, really, aggressive, to, blast, obnoxious, entertainment, in, your, guests, faces, amp, they, have, little, recourse)  \n",
       "4  (and, its, a, really, big, bad, thing, about, it)                                                                           "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'tokenized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['amp', 'rt', 'cc'])\n",
    "stop = stop - set(['no', 'not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(row):\n",
    "    return [t.text for t in row if t.text not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda row: remove_stopwords(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>[said]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>[plus, youve, added, commercials, experience, tacky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>[didnt, today, must, mean, need, take, another, trip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>[really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>[really, big, bad, thing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text  \\\n",
       "0  @VirginAmerica What @dhepburn said.                                                                                              \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.                                                         \n",
       "2  @VirginAmerica I didn't today... Must mean I need to take another trip!                                                          \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4  @VirginAmerica and it's a really big bad thing about it                                                                          \n",
       "\n",
       "                                                                                tokenized  \n",
       "0  [said]                                                                                  \n",
       "1  [plus, youve, added, commercials, experience, tacky]                                    \n",
       "2  [didnt, today, must, mean, need, take, another, trip]                                   \n",
       "3  [really, aggressive, blast, obnoxious, entertainment, guests, faces, little, recourse]  \n",
       "4  [really, big, bad, thing]                                                               "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'tokenized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_vocab_counter(row):\n",
    "    for word in row:\n",
    "        vocab_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_counter = collections.Counter()\n",
    "df['tokenized'].apply(update_vocab_counter);\n",
    "vocab = sorted(vocab_counter, key=vocab_counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12390"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the dictionary size to the top 5000 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary that map each token with their id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2id = {w:i for i, w in enumerate(vocab[:max_words])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace each token out of top 5000 with 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2id['unk'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform each token by their id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_ids(row):\n",
    "    return [w2id[w] if w in w2id else w2id['unk'] for w in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized_int'] = df['tokenized'].apply(lambda x: transform_to_ids(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = df['tokenized_int'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 21, 8.987636612021857)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lens), max(lens), np.mean(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set 20 as max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized_int'].values, df['target'].values, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need that each document contains a fixed number of tokens (20), we fill with -1 (id that represents 'unk') every token with size < 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(X_train, maxlen=maxlen, value=-1)\n",
    "x_test = pad_sequences(X_test, maxlen=maxlen, value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_y = np_utils.to_categorical(y_train)\n",
    "dummy_y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to create a neural network with a 50 dimension embedding layer as input and no hidden layers. This is equivalent as apply logistic regression over word vectors rather than one hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
    "                        Flatten(),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the models we use cross-validation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.96134572269361, 0.815186961410359)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train over full training set evaluating on test set. Best epoch is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/linear.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.9084 - acc: 0.6273Epoch 00000: val_acc improved from -inf to 0.68470, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.9025 - acc: 0.6282 - val_loss: 0.7158 - val_acc: 0.6847\n",
      "Epoch 2/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.6111 - acc: 0.7443Epoch 00001: val_acc improved from 0.68470 to 0.76202, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6083 - acc: 0.7462 - val_loss: 0.5966 - val_acc: 0.7620\n",
      "Epoch 3/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.4703 - acc: 0.8293Epoch 00002: val_acc improved from 0.76202 to 0.78798, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4678 - acc: 0.8311 - val_loss: 0.5458 - val_acc: 0.7880\n",
      "Epoch 4/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.3701 - acc: 0.8753Epoch 00003: val_acc improved from 0.78798 to 0.79344, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3699 - acc: 0.8753 - val_loss: 0.5309 - val_acc: 0.7934\n",
      "Epoch 5/5\n",
      "10700/10980 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9063Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.2978 - acc: 0.9056 - val_loss: 0.5359 - val_acc: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7de81a4f98>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple convolutional neural network with 1 conv layer (10 filters of size 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=32, input_length=maxlen),\n",
    "                        Convolution1D(10, 3, padding='same', activation='relu'),\n",
    "                        MaxPooling1D(),\n",
    "                        Flatten(),\n",
    "                        Dense(50, activation='relu'),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=conv_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.43082918098617, 2.256687530027816)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of filters to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=32, input_length=maxlen),\n",
    "                        Convolution1D(64, 3, padding='same', activation='relu'),\n",
    "                        MaxPooling1D(),\n",
    "                        Flatten(),\n",
    "                        Dense(25, activation='relu'),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=conv_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77.98683471377974, 0.9380869930210952)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a neural network that applies different filter sizes (2, 3 and 4) to word vectors, then concatenate the outputs and apply a classifier on top on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mult_conv():\n",
    "    graph_in = Input(shape=(max_words, 50))\n",
    "\n",
    "    convs = []\n",
    "    for filter_size in range(2, 5):\n",
    "        x = Convolution1D(64, filter_size, padding='same', activation='relu')(graph_in)\n",
    "        convs.append(x)\n",
    "\n",
    "    graph_out = concatenate(convs, axis=1)\n",
    "    graph_out = GlobalMaxPooling1D()(graph_out)\n",
    "    graph = Model(graph_in, graph_out)\n",
    "    \n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                    graph,\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25, activation='relu'),\n",
    "                    Dense(3, activation='softmax')])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=mult_conv, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.02512735902401, 0.7688410157779406)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/mult_conv.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10000/10980 [==========================>...] - ETA: 0s - loss: 0.8885 - acc: 0.6051Epoch 00000: val_acc improved from -inf to 0.66776, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.8752 - acc: 0.6102 - val_loss: 0.7268 - val_acc: 0.6678\n",
      "Epoch 2/5\n",
      "10100/10980 [==========================>...] - ETA: 0s - loss: 0.6250 - acc: 0.7346Epoch 00001: val_acc improved from 0.66776 to 0.77596, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6208 - acc: 0.7376 - val_loss: 0.5656 - val_acc: 0.7760\n",
      "Epoch 3/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.4895 - acc: 0.8247Epoch 00002: val_acc improved from 0.77596 to 0.78880, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4906 - acc: 0.8230 - val_loss: 0.5369 - val_acc: 0.7888\n",
      "Epoch 4/5\n",
      "10500/10980 [===========================>..] - ETA: 0s - loss: 0.3951 - acc: 0.8585Epoch 00003: val_acc improved from 0.78880 to 0.79699, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/mult_conv.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3951 - acc: 0.8587 - val_loss: 0.5599 - val_acc: 0.7970\n",
      "Epoch 5/5\n",
      "10600/10980 [===========================>..] - ETA: 0s - loss: 0.3229 - acc: 0.8866Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.3246 - acc: 0.8858 - val_loss: 0.5823 - val_acc: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d64be6518>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mult_conv()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load pre-trained word embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_dir = '/home/martinpella/Downloads/GloVe/Wikipedia2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(glove_dir + '/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb_matrix(max_words, embedding_dim):\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    found = 0\n",
    "    for word, i in w2id.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if i < max_words:\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                found += 1\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix, found = create_emb_matrix(max_words, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4676"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential([Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
    "                        Flatten(),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.79746936570686, 0.7629237463005589)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/linear_glove.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.9241 - acc: 0.6159Epoch 00000: val_acc improved from -inf to 0.67869, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.9121 - acc: 0.6187 - val_loss: 0.7213 - val_acc: 0.6787\n",
      "Epoch 2/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.6166 - acc: 0.7404Epoch 00001: val_acc improved from 0.67869 to 0.75710, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.6138 - acc: 0.7420 - val_loss: 0.5987 - val_acc: 0.7571\n",
      "Epoch 3/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.4713 - acc: 0.8212Epoch 00002: val_acc improved from 0.75710 to 0.78415, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.4717 - acc: 0.8214 - val_loss: 0.5473 - val_acc: 0.7842\n",
      "Epoch 4/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.3734 - acc: 0.8757Epoch 00003: val_acc improved from 0.78415 to 0.79262, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/linear_glove.hdf5\n",
      "10980/10980 [==============================] - 0s - loss: 0.3723 - acc: 0.8745 - val_loss: 0.5311 - val_acc: 0.7926\n",
      "Epoch 5/5\n",
      "10300/10980 [===========================>..] - ETA: 0s - loss: 0.2997 - acc: 0.9046Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 0s - loss: 0.2987 - acc: 0.9054 - val_loss: 0.5351 - val_acc: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d5aef5a90>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Multiple Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mult_conv():\n",
    "    graph_in = Input(shape=(max_words, 100))\n",
    "\n",
    "    convs = []\n",
    "    for filter_size in range(2, 5):\n",
    "        x = Convolution1D(64, filter_size, padding='same', activation='relu')(graph_in)\n",
    "        convs.append(x)\n",
    "\n",
    "    graph_out = concatenate(convs, axis=1)\n",
    "    graph_out = GlobalMaxPooling1D()(graph_out)\n",
    "    graph = Model(graph_in, graph_out)\n",
    "    \n",
    "    model = Sequential([Embedding(max_words, 100, input_length=maxlen),\n",
    "                    graph,\n",
    "                    Dropout(0.5),\n",
    "                    Dense(25, activation='relu'),\n",
    "                    Dense(3, activation='softmax')])\n",
    "    \n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=mult_conv, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.87934992381346, 0.8683407303678339)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        LSTM(25),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=simple_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.37855569354937, 0.6784259807207694)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = f'{PATH}/results/lstm_glove.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.6480Epoch 00000: val_acc improved from -inf to 0.74098, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 8s - loss: 0.8144 - acc: 0.6487 - val_loss: 0.6448 - val_acc: 0.7410\n",
      "Epoch 2/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.7998Epoch 00001: val_acc improved from 0.74098 to 0.77842, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.5404 - acc: 0.8002 - val_loss: 0.5651 - val_acc: 0.7784\n",
      "Epoch 3/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8510Epoch 00002: val_acc improved from 0.77842 to 0.77896, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.3967 - acc: 0.8510 - val_loss: 0.5470 - val_acc: 0.7790\n",
      "Epoch 4/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8855Epoch 00003: val_acc improved from 0.77896 to 0.78333, saving model to /home/martinpella/Projects/nlp/twitter-airline/results/lstm_glove.hdf5\n",
      "10980/10980 [==============================] - 2s - loss: 0.3186 - acc: 0.8848 - val_loss: 0.5759 - val_acc: 0.7833\n",
      "Epoch 5/5\n",
      "10900/10980 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9047Epoch 00004: val_acc did not improve\n",
      "10980/10980 [==============================] - 2s - loss: 0.2687 - acc: 0.9043 - val_loss: 0.6196 - val_acc: 0.7806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d1ed61f28>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = simple_lstm()\n",
    "model.fit(x_train, dummy_y, validation_data=(x_test, dummy_y_test), epochs=5, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_lstm():\n",
    "    model = Sequential([Embedding(max_words, 50, input_length=maxlen),\n",
    "                        Bidirectional(LSTM(25)),\n",
    "                        Dense(3, activation='softmax')])\n",
    "\n",
    "    model.layers[0].set_weights = [embedding_matrix]\n",
    "    model.layers[0].trainable = True\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=bi_lstm, epochs=5, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77.3858536422894, 0.780207659164508)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()*100, results.std()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
